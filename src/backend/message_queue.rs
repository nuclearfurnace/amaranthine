// Copyright (c) 2018 Nuclear Furnace
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
use backend::processor::{ProcessorError, RequestProcessor};
use bytes::BytesMut;
use common::Message;
use futures::{
    future::ok,
    prelude::*,
    sync::{
        mpsc::{self, UnboundedReceiver, UnboundedSender},
        oneshot::{self, Sender},
    },
};
use metrics::{get_sink, Metrics};
use slab::Slab;
use std::collections::VecDeque;
use tokio::io::{write_all, AsyncWrite};

#[derive(Debug, PartialEq)]
pub enum MessageState {
    /// An unfragmented, standalone message.
    ///
    /// A filled variant of this state can be immediately sent off to the client.
    Standalone,

    /// An unfragmented, standalone message that is _also_ immediately available.
    ///
    /// While normal messages have to be processed before a response, these messages are available
    /// to send as soon as they're enqueued.
    Inline,

    /// A fragmented message.
    ///
    /// This represents a discrete fragment of a parent message.  The buffer represents arbitrary
    /// data that is used to identify the parent message.  Given that fragments may not have the
    /// information any longer, we keep track of it in the message state.
    ///
    /// The integers provide the index of the given fragment and the overall count of fragments
    /// within the parent message.
    Fragmented(BytesMut, usize, usize),

    /// A streaming fragmented message.
    ///
    /// This represents a discrete fragment of a parent message.  The key difference is that the
    /// parent message is "streamable."  This is usually the case for get operations, where, as
    /// long as the fragments are in order, they can be streamed back to the client as they're
    /// available.  This is in contrast to some other fragmented messages, where the response must
    /// be generated by the sum of all the parts.
    ///
    /// The optional buffer represents a header that can be sent before the actual fragment.  This
    /// allows sending any response data that is needed to coalesce the fragments into a meaningful
    /// response to the client.
    StreamingFragmented(Option<BytesMut>),
}

/// A proxy to a queued message slot.
///
/// This holds the actual request which is tied to the response slot, as well as the slot and the
/// return channel so that callers can hand over the response and shuttle it back to the message
/// queue without explicit knowledge of how to get it there.
#[derive(Debug)]
pub struct QueuedMessage<M> {
    msg: Option<M>,
    slot: usize,
    response_tx: UnboundedSender<(usize, M)>,
}

impl<M> Message for QueuedMessage<M>
where
    M: Message,
{
    fn key(&self) -> &[u8] { self.msg.as_ref().unwrap().key() }

    fn is_inline(&self) -> bool { self.msg.as_ref().unwrap().is_inline() }

    fn into_buf(mut self) -> BytesMut { self.msg.take().unwrap().into_buf() }
}

impl<M> QueuedMessage<M> {
    pub fn new(msg: M, slot: usize, response_tx: UnboundedSender<(usize, M)>) -> QueuedMessage<M> {
        QueuedMessage {
            msg: Some(msg),
            slot,
            response_tx,
        }
    }

    /// Consumes the request message and returns it to the caller.
    pub fn consume_inner(&mut self) -> Option<M> { self.msg.take() }

    /// Sends back a message to the queue to the assigned slot.
    pub fn respond(&self, response: M) { let _ = self.response_tx.unbounded_send((self.slot, response)); }
}

enum QueueControlMessage<M> {
    /// Enqueues a set of messages in the message queue, returning the allocated slots for all
    /// expanded messages in the original list.
    ///
    /// Messages are defragmented at this stage so the slot allocation can account for them, so for
    /// multi-backend messages, the number of returned messages will be greater than the number of
    /// input messages.
    Enqueue(Vec<M>, Sender<Vec<QueuedMessage<M>>>),
}

/// A message queue provides an ordered queue of message placeholders.  These placeholders are, in
/// essence, futures, representing a to-be-received response to some request.
///
/// Downstream tasks that processes these requests and generate responses have a fast path back to
/// the queue in order to hand back the response so it can be sent to the client.
pub struct MessageQueue<P>
where
    P: RequestProcessor + Send + 'static,
{
    input_closed: bool,
    output_closed: bool,

    // Processor that provides fragmentation capabilities.
    processor: P,

    // Channel that allows the control plane to interact with us at runtime.
    control_rx: UnboundedReceiver<QueueControlMessage<P::Message>>,

    // Holds all message slots, and stores the slot IDs in order of the messages tied to them.
    slot_order: VecDeque<(usize, MessageState)>,
    slots: Slab<Option<P::Message>>,

    // Channel for backends to notify us when a response has been generated.
    responses_rx: UnboundedReceiver<(usize, P::Message)>,
    responses_tx: Option<UnboundedSender<(usize, P::Message)>>,

    buf_tx: UnboundedSender<Vec<BytesMut>>,
}

impl<P> MessageQueue<P>
where
    P: RequestProcessor + Send + 'static,
    P::Message: Message,
{
    pub fn new<T>(processor: P, tx: T) -> (MessageQueue<P>, MessageQueueControlPlane<P>)
    where
        T: AsyncWrite + Send + 'static,
    {
        let (control_tx, control_rx) = mpsc::unbounded();
        let (responses_tx, responses_rx) = mpsc::unbounded();
        let (buf_tx, buf_rx) = mpsc::unbounded();

        // Spinning up our buffer shuttling task.
        let f = buf_rx
            .fold(tx, |tx, ibufs| {
                let mut obuf = BytesMut::new();
                for ibuf in ibufs {
                    obuf.unsplit(ibuf);
                }

                write_all(tx, obuf).map_err(|_| ()).map(|(tx, _)| tx)
            }).and_then(|_| {
                debug!("[message queue buf tx] closing!");
                ok(())
            }).map_err(|_| {
                get_sink().increment(Metrics::ClientTxErrors);
            }).map(|_| ());
        tokio::spawn(f);

        let data = MessageQueue {
            input_closed: false,
            output_closed: false,

            processor,
            control_rx,

            slot_order: VecDeque::new(),
            slots: Slab::new(),

            responses_rx,
            responses_tx: Some(responses_tx),

            buf_tx,
        };

        let control = MessageQueueControlPlane { control_tx };

        (data, control)
    }

    /// Determines if a given slot has been filled yet.
    ///
    /// The `index` given is the ordering index, i.e. `self.slot_order`, so an `index` of `0` is
    /// the first slot, `1` is the second slot, etc.  It is unrelated to the underlying index in
    /// the slab.
    fn is_slot_ready(&self, index: usize) -> bool {
        match self.slot_order.get(index) {
            None => false,
            Some((slot_id, _)) => {
                match self.slots.get(*slot_id) {
                    Some(Some(_)) => true,
                    _ => false,
                }
            },
        }
    }

    fn get_next_response(&mut self) -> Poll<Option<BytesMut>, ProcessorError> {
        // If we have no slots in play, then we're "technically" done, although downstream code
        // will likely boolean-and this with whether or not the response stream is closed.
        if self.slot_order.is_empty() {
            return Ok(Async::Ready(None));
        }

        // See if the next slot is even ready yet.  If it's not, then we can't do anything.
        if !self.is_slot_ready(0) {
            return Ok(Async::NotReady);
        }

        // If we have an immediately available response aka a standalone message or streaming
        // fragment, just return it.
        let has_immediate = match self.slot_order.front() {
            None => false,
            Some((slot_id, state)) => {
                match self.slots.get(*slot_id) {
                    Some(_) => {
                        match state {
                            MessageState::Standalone => true,
                            MessageState::Inline => true,
                            MessageState::StreamingFragmented(_) => true,
                            MessageState::Fragmented(_, _, _) => false,
                        }
                    },
                    None => false,
                }
            },
        };

        if has_immediate {
            let (slot_id, state) = self.slot_order.pop_front().unwrap();
            let slot = self.slots.remove(slot_id).unwrap();

            let buf = match state {
                MessageState::Standalone | MessageState::Inline => slot.into_buf(),
                MessageState::StreamingFragmented(header) => {
                    match header {
                        Some(mut header_buf) => {
                            header_buf.unsplit(slot.into_buf());
                            header_buf
                        },
                        None => slot.into_buf(),
                    }
                },
                _ => unreachable!(),
            };

            return Ok(Async::Ready(Some(buf)));
        }

        // Now we know that the next slot has been fulfilled, and that it's a fragmented message.
        // Let's peek at the slot to grab the fragment count, and then we can loop through to see
        // if all the fragments have completed and are ready to be coalesced.
        let fragment_count = match self.slot_order.front() {
            None => unreachable!(),
            Some((_, state)) => {
                match state {
                    MessageState::Fragmented(_, _, count) => *count,
                    _ => unreachable!(),
                }
            },
        };

        for index in 0..fragment_count {
            if !self.is_slot_ready(index) {
                return Ok(Async::NotReady);
            }
        }

        // We have all the slots filled and ready to coalesce.  Pull out the fragments!
        let mut fragments = Vec::new();
        for _ in 0..fragment_count {
            let (slot_id, state) = self.slot_order.pop_front().unwrap();
            let msg = self.slots.remove(slot_id);
            fragments.push((state, msg.unwrap()));
        }

        let msg = self.processor.defragment_messages(fragments)?;
        Ok(Async::Ready(Some(msg.into_buf())))
    }
}

impl<P> Future for MessageQueue<P>
where
    P: RequestProcessor + Send + 'static,
    P::Message: Message,
{
    type Error = ();
    type Item = ();

    fn poll(&mut self) -> Poll<Self::Item, Self::Error> {
        debug!("[message queue] poll enter");

        if self.input_closed && self.output_closed {
            debug!("[message queue] input/output closed, shutting down");
            return Ok(Async::Ready(()));
        }

        // First, go through and process any control messages.
        loop {
            debug!("[message queue] enter control process loop");

            // Only service the control loop if we're not closed.   Otherwise, we're just draining
            // responses as they come in for the client.
            if self.input_closed {
                break;
            }

            match self.control_rx.poll() {
                Ok(Async::Ready(Some(msg))) => {
                    match msg {
                        QueueControlMessage::Enqueue(msgs, tx) => {
                            match self.processor.fragment_messages(msgs) {
                                Ok(fmsgs) => {
                                    debug!("[message queue] enqueueing {} message fragments", fmsgs.len());

                                    let mut qmsgs = Vec::new();
                                    for (msg_state, msg) in fmsgs {
                                        if msg_state == MessageState::Inline {
                                            let slot_id = self.slots.insert(Some(msg));
                                            self.slot_order.push_back((slot_id, msg_state));
                                        } else {
                                            let slot_id = self.slots.insert(None);
                                            self.slot_order.push_back((slot_id, msg_state));
                                            qmsgs.push(QueuedMessage::new(
                                                msg,
                                                slot_id,
                                                self.responses_tx.as_ref().unwrap().clone(),
                                            ));
                                        }
                                    }

                                    match tx.send(qmsgs) {
                                        Ok(()) => {},
                                        Err(_) => panic!("unable to send to enqueue return tx"),
                                    }
                                },
                                // TODO: how do we propagate this back to the main client loop?
                                Err(_) => return Err(()),
                            }
                        },
                    }
                },
                Ok(Async::NotReady) => break,
                Ok(Async::Ready(None)) => {
                    debug!("[message queue] control plane dropped, closing input");
                    self.input_closed = true;

                    // We have to drop our copy of the sending side of the responses channel
                    // otherwise the receiver will never know to actually close the stream.
                    let _responses_tx = self.responses_tx.take();
                    drop(_responses_tx);

                    break;
                },
                // Unbounded receivers never actually throw an error.
                Err(_) => unreachable!(),
            }
        }

        // Pull in any responses we've gotten.
        loop {
            debug!("[message queue] enter response process loop");

            match self.responses_rx.poll() {
                Ok(Async::Ready(Some((id, msg)))) => {
                    let slot = self.slots.get_mut(id).unwrap();
                    slot.replace(msg);
                },
                Ok(Async::NotReady) | Ok(Async::Ready(None)) => break,
                // Unbounded receivers never actually throw an error.
                Err(_) => unreachable!(),
            }
        }

        // Now just go through our slots and see if we have any that are ready to send!
        let mut bufs = Vec::new();

        loop {
            debug!("[message queue] enter buf send loop");

            match self.get_next_response() {
                Ok(Async::Ready(Some(buf))) => bufs.push(buf),
                Ok(Async::NotReady) => break,
                Ok(Async::Ready(None)) => {
                    if self.input_closed {
                        // We've cleared everything out of the queue.  Time to shutdown for real.
                        // Notify ourselves so that we get polled one more time, which should
                        // trigger the full shutdown when it checks if input/output is closed.
                        self.output_closed = true;
                        debug!("[message queue] no more slots present, closing output");
                    }

                    break;
                },
                // TODO: how do we propagate this back to the main client loop?
                Err(_) => return Err(()),
            }
        }

        if !bufs.is_empty() {
            self.buf_tx.unbounded_send(bufs).expect("buf tx send should not fail");
        }

        // Fast path to close the message queue if we're shutting down.
        if self.input_closed && self.output_closed {
            debug!("[message queue] input/output closed, shutting down (fast path)");
            return Ok(Async::Ready(()));
        }

        Ok(Async::NotReady)
    }
}

impl<P> Drop for MessageQueue<P>
where
    P: RequestProcessor + Send,
{
    fn drop(&mut self) {
        debug!("[message queue] dropping!");
    }
}

/// Control surface for a message queue.
///
/// Allows operating with a message queue at runtime, once it has been spawned and no longer owned.
pub struct MessageQueueControlPlane<P>
where
    P: RequestProcessor + Send + 'static,
{
    control_tx: UnboundedSender<QueueControlMessage<P::Message>>,
}

impl<P> MessageQueueControlPlane<P>
where
    P: RequestProcessor + Send + 'static,
{
    /// Queues a set of messages in this message queue, allocating a slot for a response for each
    /// message.  Queued messages have a reference back to this messge queue in order to notify it.
    pub fn enqueue(&mut self, msgs: Vec<P::Message>) -> impl Future<Item = Vec<QueuedMessage<P::Message>>, Error = ()> {
        let (tx, rx) = oneshot::channel();

        self.control_tx
            .unbounded_send(QueueControlMessage::Enqueue(msgs, tx))
            .expect("unable to send to unbounded control_tx");
        rx.map_err(|_| ())
    }
}

impl<P> Drop for MessageQueueControlPlane<P>
where
    P: RequestProcessor + Send,
{
    fn drop(&mut self) {
        debug!("[message queue control plane] dropping!");
    }
}
